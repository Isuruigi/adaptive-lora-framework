{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Adaptive LoRA - Training Notebook\n",
                "\n",
                "This notebook trains LoRA adapters for the Adaptive Multi-Adapter system.\n",
                "\n",
                "**Requirements:**\n",
                "- Google Colab Pro (A100 recommended)\n",
                "- HuggingFace account with token\n",
                "\n",
                "**Training time:** ~2 hours per adapter on A100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install torch transformers accelerate bitsandbytes peft datasets wandb -q\n",
                "print('Dependencies installed!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration - Use Colab Secrets for tokens\n",
                "import os\n",
                "from huggingface_hub import login\n",
                "from google.colab import userdata\n",
                "\n",
                "# Login to HuggingFace using Colab secrets\n",
                "HF_TOKEN = userdata.get('HF_TOKEN')  # Set in Colab: Settings > Secrets\n",
                "login(HF_TOKEN)\n",
                "\n",
                "# Weights & Biases (optional)\n",
                "os.environ['WANDB_PROJECT'] = 'adaptive-lora'\n",
                "\n",
                "# Model config\n",
                "BASE_MODEL = 'meta-llama/Llama-3.2-3B-Instruct'\n",
                "ADAPTER_NAME = 'code'  # Options: reasoning, code, creative, analysis\n",
                "\n",
                "print(f'Training {ADAPTER_NAME} adapter on {BASE_MODEL}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model with 4-bit quantization\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type='nf4',\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map='auto',\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "print(f'Model loaded! GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure LoRA\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "# LoRA config - adjust per adapter type\n",
                "lora_configs = {\n",
                "    'reasoning': {'r': 32, 'lora_alpha': 64},\n",
                "    'code': {'r': 64, 'lora_alpha': 128},\n",
                "    'creative': {'r': 16, 'lora_alpha': 32},\n",
                "    'analysis': {'r': 32, 'lora_alpha': 64}\n",
                "}\n",
                "\n",
                "config = LoraConfig(\n",
                "    r=lora_configs[ADAPTER_NAME]['r'],\n",
                "    lora_alpha=lora_configs[ADAPTER_NAME]['lora_alpha'],\n",
                "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
                "    lora_dropout=0.05,\n",
                "    bias='none',\n",
                "    task_type='CAUSAL_LM'\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load training data\n",
                "from datasets import load_dataset\n",
                "\n",
                "dataset = load_dataset('tatsu-lab/alpaca', split='train[:1000]')\n",
                "\n",
                "def format_prompt(example):\n",
                "    if example.get('input'):\n",
                "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
                "    else:\n",
                "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
                "    return {'text': text}\n",
                "\n",
                "dataset = dataset.map(format_prompt)\n",
                "\n",
                "def tokenize(example):\n",
                "    return tokenizer(example['text'], truncation=True, max_length=512, padding='max_length')\n",
                "\n",
                "dataset = dataset.map(tokenize, batched=True)\n",
                "print(f'Dataset size: {len(dataset)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=f'./models/{ADAPTER_NAME}',\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=8,\n",
                "    learning_rate=2e-4,\n",
                "    lr_scheduler_type='cosine',\n",
                "    warmup_ratio=0.1,\n",
                "    logging_steps=10,\n",
                "    save_strategy='epoch',\n",
                "    fp16=True,\n",
                "    optim='paged_adamw_8bit',\n",
                "    report_to='none'\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=dataset,\n",
                "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
                ")\n",
                "\n",
                "print('Ready to train!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train!\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save adapter\n",
                "model.save_pretrained(f'./models/{ADAPTER_NAME}')\n",
                "print(f'Adapter saved to ./models/{ADAPTER_NAME}')\n",
                "\n",
                "# Push to Hub (optional)\n",
                "# model.push_to_hub(f'your-username/adaptive-lora-{ADAPTER_NAME}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the adapter\n",
                "model.eval()\n",
                "\n",
                "test_prompt = '### Instruction:\\nExplain what machine learning is.\\n\\n### Response:\\n'\n",
                "inputs = tokenizer(test_prompt, return_tensors='pt').to('cuda')\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
                "\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. **Train other adapters** - Change `ADAPTER_NAME` and rerun\n",
                "2. **Download adapters** - Save to Google Drive or push to HuggingFace\n",
                "3. **Train Router** - Use `experiments/train_router.py`\n",
                "4. **Deploy** - Use the trained adapters with the serving infrastructure"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Adaptive LoRA - Simple API Server\n",
                "**Uses HuggingFace Inference API - No model loading needed**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Install\n",
                "!pip install gradio huggingface_hub -q\n",
                "print('Done!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Setup - Get your token from https://huggingface.co/settings/tokens\n",
                "from huggingface_hub import InferenceClient\n",
                "import gradio as gr\n",
                "from google.colab import userdata\n",
                "\n",
                "# Use Colab secrets (recommended) or paste your token\n",
                "HF_TOKEN = userdata.get('HF_TOKEN')  # Set in Colab Secrets\n",
                "client = InferenceClient(token=HF_TOKEN)\n",
                "print('Ready!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: Test\n",
                "response = client.text_generation(\n",
                "    'Explain AI in one sentence:',\n",
                "    model='mistralai/Mistral-7B-Instruct-v0.2',\n",
                "    max_new_tokens=100\n",
                ")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 4: Launch Server with Public URL\n",
                "def generate(prompt, max_tokens=200):\n",
                "    return client.text_generation(\n",
                "        prompt,\n",
                "        model='mistralai/Mistral-7B-Instruct-v0.2',\n",
                "        max_new_tokens=int(max_tokens)\n",
                "    )\n",
                "\n",
                "demo = gr.Interface(\n",
                "    fn=generate,\n",
                "    inputs=[gr.Textbox(label='Prompt', lines=3), gr.Slider(50, 500, value=200, label='Max Tokens')],\n",
                "    outputs=gr.Textbox(label='Response', lines=10),\n",
                "    title='Adaptive LoRA API'\n",
                ")\n",
                "\n",
                "print('Starting server...')\n",
                "demo.launch(share=True, debug=True)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
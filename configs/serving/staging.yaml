# Staging Serving Configuration
# Settings for staging/testing environment

server:
  name: "adaptive-lora-staging"
  environment: "staging"
  
# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  workers: 2
  timeout: 300
  reload: false
  
  # CORS - more permissive for testing
  cors:
    enabled: true
    origins:
      - "*"
    allow_credentials: true
    allow_methods: ["*"]
    allow_headers: ["*"]

# Model Configuration
model:
  base_model: "meta-llama/Llama-3-8B"
  adapters_path: "./models/adapters"
  
  # Quantization
  quantization:
    enabled: true
    bits: 4
    quant_type: "nf4"
    double_quant: true
  
  # vLLM Settings
  vllm:
    enabled: true
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    max_model_len: 4096
    enforce_eager: false

# Inference Settings
inference:
  max_tokens: 2048
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  
  # Batching
  batch:
    enabled: true
    max_batch_size: 16
    max_wait_time_ms: 200
  
  # Caching
  cache:
    enabled: true
    type: "memory"
    ttl_seconds: 1800
    max_size: 5000

# Rate Limiting - more relaxed
rate_limiting:
  enabled: true
  
  tiers:
    default:
      requests_per_minute: 120
      tokens_per_minute: 200000

# Authentication
auth:
  enabled: true
  
  jwt:
    enabled: true
    algorithm: "HS256"
    expiry_minutes: 120
    
  api_key:
    enabled: true
    header_name: "X-API-Key"

# Redis Configuration
redis:
  url: "redis://redis-staging:6379"
  db: 0
  max_connections: 50

# Monitoring
monitoring:
  prometheus:
    enabled: true
    port: 9090
    path: "/metrics"
  
  health_check:
    enabled: true
    path: "/health"
    include_details: true

# Logging - more verbose
logging:
  level: "DEBUG"
  format: "json"
  file: "/var/log/adaptive-lora/staging.log"

# Security
security:
  ssl:
    enabled: false
  max_request_size_mb: 20

# Scaling
scaling:
  min_replicas: 1
  max_replicas: 4
  
  autoscaling:
    enabled: true
    target_cpu_utilization: 75
